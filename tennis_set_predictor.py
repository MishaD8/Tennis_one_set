import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import xgboost as xgb
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

class EnhancedTennisPredictor:
    def __init__(self, model_dir="tennis_models"):
        self.model_dir = model_dir
        self.scaler = RobustScaler()  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
        self.models = {}
        self.feature_importance = {}
        self.feature_columns = []
        self.ensemble_weights = {}
        self.validation_scores = {}
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –£–õ–£–ß–®–ï–ù–û: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å feature engineering
        """
        features = df.copy()
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–≥—Ä–æ–∫–∞
        player_basic = [
            'player_rank', 'player_age', 'opponent_rank', 'opponent_age'
        ]
        
        # –§–æ—Ä–º–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        form_features = [
            'player_recent_matches_count', 'player_recent_win_rate', 
            'player_recent_sets_win_rate', 'player_form_trend',
            'player_days_since_last_match'
        ]
        
        # –ü–æ–∫—Ä—ã—Ç–∏–µ
        surface_features = [
            'player_surface_matches_count', 'player_surface_win_rate',
            'player_surface_advantage', 'player_surface_sets_rate', 
            'player_surface_experience'
        ]
        
        # –û—á–Ω—ã–µ –≤—Å—Ç—Ä–µ—á–∏
        h2h_features = [
            'h2h_matches', 'h2h_win_rate', 'h2h_recent_form',
            'h2h_sets_advantage', 'days_since_last_h2h'
        ]
        
        # –î–∞–≤–ª–µ–Ω–∏–µ —Ç—É—Ä–Ω–∏—Ä–∞
        pressure_features = [
            'tournament_importance', 'round_pressure', 'total_pressure',
            'is_high_pressure_tournament'
        ]
        
        self.feature_columns = (player_basic + form_features + surface_features + 
                               h2h_features + pressure_features)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        available_features = [col for col in self.feature_columns if col in features.columns]
        missing_features = [col for col in self.feature_columns if col not in features.columns]
        
        if missing_features:
            print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_enhanced = features[available_features].copy()
        
        # Feature engineering - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'player_rank' in features_enhanced.columns and 'opponent_rank' in features_enhanced.columns:
            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∏–≥—Ä–æ–∫–æ–≤
            features_enhanced['rank_difference'] = features_enhanced['opponent_rank'] - features_enhanced['player_rank']
            features_enhanced['rank_ratio'] = features_enhanced['player_rank'] / (features_enhanced['opponent_rank'] + 1)
            
        if 'player_recent_win_rate' in features_enhanced.columns and 'h2h_win_rate' in features_enhanced.columns:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞
            features_enhanced['combined_form'] = (features_enhanced['player_recent_win_rate'] * 0.7 + 
                                                features_enhanced['h2h_win_rate'] * 0.3)
        
        if 'player_surface_advantage' in features_enhanced.columns and 'total_pressure' in features_enhanced.columns:
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –¥–∞–≤–ª–µ–Ω–∏—é –Ω–∞ –ø–æ–∫—Ä—ã—Ç–∏–∏
            features_enhanced['surface_pressure_interaction'] = (features_enhanced['player_surface_advantage'] * 
                                                               features_enhanced['total_pressure'])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        features_enhanced = features_enhanced.fillna(features_enhanced.median())
        
        return features_enhanced
    
    def create_neural_network(self, input_dim: int) -> keras.Model:
        """
        –£–õ–£–ß–®–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        """
        model = Sequential([
            # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
            Dense(256, activation='relu', input_dim=input_dim),
            BatchNormalization(),
            Dropout(0.3),
            
            # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.25),
            
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            
            Dense(32, activation='relu'),
            BatchNormalization(),
            Dropout(0.15),
            
            Dense(16, activation='relu'),
            Dropout(0.1),
            
            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Dense(1, activation='sigmoid')
        ])
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º learning rate
        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall', 'AUC']
        )
        
        return model
    
    def train_ensemble_models(self, X_train: pd.DataFrame, y_train: pd.Series, 
                        X_val: pd.DataFrame, y_val: pd.Series) -> Dict:
        """
        –ù–û–í–û–ï: –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        """
        print("üß† –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        models_performance = {}
        
        # 1. –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
        print("üî∏ –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        nn_model = self.create_neural_network(X_train_scaled.shape[1])
        
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
        ]
        
        history = nn_model.fit(
            X_train_scaled, y_train,
            validation_data=(X_val_scaled, y_val),
            epochs=150,
            batch_size=64,
            verbose=0,
            callbacks=callbacks
        )
        
        nn_pred = nn_model.predict(X_val_scaled).flatten()
        nn_auc = roc_auc_score(y_val, nn_pred)
        models_performance['neural_network'] = nn_auc
        self.models['neural_network'] = nn_model
        
        print(f"‚úÖ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å: AUC = {nn_auc:.4f}")
        
        # 2. XGBoost
        print("üî∏ –û–±—É—á–µ–Ω–∏–µ XGBoost...")
        xgb_model = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='auc'
        )
        
        xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=20,
            verbose=False
        )
        
        xgb_pred = xgb_model.predict_proba(X_val)[:, 1]
        xgb_auc = roc_auc_score(y_val, xgb_pred)
        models_performance['xgboost'] = xgb_auc
        self.models['xgboost'] = xgb_model
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_importance['xgboost'] = dict(zip(X_train.columns, xgb_model.feature_importances_))
        
        print(f"‚úÖ XGBoost: AUC = {xgb_auc:.4f}")
        
        # 3. Random Forest
        print("üî∏ –û–±—É—á–µ–Ω–∏–µ Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict_proba(X_val)[:, 1]
        rf_auc = roc_auc_score(y_val, rf_pred)
        models_performance['random_forest'] = rf_auc
        self.models['random_forest'] = rf_model
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_importance['random_forest'] = dict(zip(X_train.columns, rf_model.feature_importances_))
        
        print(f"‚úÖ Random Forest: AUC = {rf_auc:.4f}")
        
        # 4. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
        print("üî∏ –û–±—É—á–µ–Ω–∏–µ Gradient Boosting...")
        gb_model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=5,
            random_state=42
        )
        
        gb_model.fit(X_train, y_train)
        gb_pred = gb_model.predict_proba(X_val)[:, 1]
        gb_auc = roc_auc_score(y_val, gb_pred)
        models_performance['gradient_boosting'] = gb_auc
        self.models['gradient_boosting'] = gb_model
        
        print(f"‚úÖ Gradient Boosting: AUC = {gb_auc:.4f}")
        
        # 5. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (–∫–∞–∫ baseline)
        print("üî∏ –û–±—É—á–µ–Ω–∏–µ Logistic Regression...")
        lr_model = LogisticRegression(random_state=42, max_iter=1000)
        lr_model.fit(X_train_scaled, y_train)
        lr_pred = lr_model.predict_proba(X_val_scaled)[:, 1]
        lr_auc = roc_auc_score(y_val, lr_pred)
        models_performance['logistic_regression'] = lr_auc
        self.models['logistic_regression'] = lr_model
        
        print(f"‚úÖ Logistic Regression: AUC = {lr_auc:.4f}")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        total_performance = sum(models_performance.values())
        self.ensemble_weights = {
            model: performance / total_performance 
            for model, performance in models_performance.items()
        }
        
        print(f"\nüìä –í–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è:")
        for model, weight in self.ensemble_weights.items():
            print(f"‚Ä¢ {model}: {weight:.3f}")
        
        self.validation_scores = models_performance
        
        return models_performance
    
    def predict_probability(self, X: pd.DataFrame) -> np.ndarray:
        """
        –ù–û–í–û–ï: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è
        """
        X_features = self.prepare_features(X)
        X_scaled = self.scaler.transform(X_features)
        
        predictions = []
        weights = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name, model in self.models.items():
            if model_name == 'neural_network' or model_name == 'logistic_regression':
                pred = model.predict(X_scaled).flatten()
            else:
                pred = model.predict_proba(X_features)[:, 1]
            
            predictions.append(pred)
            weights.append(self.ensemble_weights[model_name])
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
        ensemble_prediction = np.average(predictions, weights=weights, axis=0)
        
        return ensemble_prediction
    
    def evaluate_models(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """
        –ù–û–í–û–ï: –ü–æ–¥—Ä–æ–±–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        print("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        X_test_features = self.prepare_features(X_test)
        X_test_scaled = self.scaler.transform(X_test_features)
        
        results = {}
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name, model in self.models.items():
            if model_name == 'neural_network' or model_name == 'logistic_regression':
                y_pred_proba = model.predict(X_test_scaled).flatten()
            else:
                y_pred_proba = model.predict_proba(X_test_features)[:, 1]
            
            y_pred = (y_pred_proba > 0.5).astype(int)
            
            auc_score = roc_auc_score(y_test, y_pred_proba)
            accuracy = (y_pred == y_test).mean()
            
            results[model_name] = {
                'auc': auc_score,
                'accuracy': accuracy,
                'predictions': y_pred_proba
            }
        
        # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_pred = self.predict_probability(X_test)
        ensemble_pred_binary = (ensemble_pred > 0.5).astype(int)
        
        ensemble_auc = roc_auc_score(y_test, ensemble_pred)
        ensemble_accuracy = (ensemble_pred_binary == y_test).mean()
        
        results['ensemble'] = {
            'auc': ensemble_auc,
            'accuracy': ensemble_accuracy,
            'predictions': ensemble_pred
        }
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
        print("=" * 50)
        for model_name, metrics in results.items():
            print(f"{model_name:20}: AUC = {metrics['auc']:.4f}, Accuracy = {metrics['accuracy']:.4f}")
        
        return results
    
    def plot_feature_importance(self, top_n: int = 20):
        """
        –ù–û–í–û–ï: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if not self.feature_importance:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        axes = axes.ravel()
        
        plot_idx = 0
        for model_name, importance in self.feature_importance.items():
            if plot_idx >= 4:
                break
                
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:top_n]
            features, values = zip(*sorted_features)
            
            axes[plot_idx].barh(range(len(features)), values)
            axes[plot_idx].set_yticks(range(len(features)))
            axes[plot_idx].set_yticklabels(features)
            axes[plot_idx].set_title(f'Feature Importance - {model_name}')
            axes[plot_idx].set_xlabel('Importance')
            
            plot_idx += 1
        
        plt.tight_layout()
        plt.show()
    
    def plot_model_comparison(self, results: Dict):
        """
        –ù–û–í–û–ï: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        """
        models = list(results.keys())
        auc_scores = [results[model]['auc'] for model in models]
        accuracy_scores = [results[model]['accuracy'] for model in models]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # AUC scores
        bars1 = ax1.bar(models, auc_scores, color='skyblue', alpha=0.7)
        ax1.set_title('Model Comparison - AUC Score')
        ax1.set_ylabel('AUC Score')
        ax1.set_ylim(0.5, 1.0)
        ax1.tick_params(axis='x', rotation=45)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, score in zip(bars1, auc_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom')
        
        # Accuracy scores
        bars2 = ax2.bar(models, accuracy_scores, color='lightcoral', alpha=0.7)
        ax2.set_title('Model Comparison - Accuracy')
        ax2.set_ylabel('Accuracy')
        ax2.set_ylim(0.5, 1.0)
        ax2.tick_params(axis='x', rotation=45)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, score in zip(bars2, accuracy_scores):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
    
    def save_models(self):
        """
        –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–∞–ª–µ—Ä
        joblib.dump(self.scaler, os.path.join(self.model_dir, 'scaler.pkl'))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for model_name, model in self.models.items():
            if model_name == 'neural_network':
                model.save(os.path.join(self.model_dir, f'{model_name}.h5'))
            else:
                joblib.dump(model, os.path.join(self.model_dir, f'{model_name}.pkl'))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'feature_columns': self.feature_columns,
            'ensemble_weights': self.ensemble_weights,
            'validation_scores': self.validation_scores,
            'feature_importance': self.feature_importance
        }
        
        import json
        with open(os.path.join(self.model_dir, 'metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        print(f"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.model_dir}")
    
    def load_models(self):
        """
        –ù–û–í–û–ï: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–∞–ª–µ—Ä
        self.scaler = joblib.load(os.path.join(self.model_dir, 'scaler.pkl'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        import json
        with open(os.path.join(self.model_dir, 'metadata.json'), 'r') as f:
            metadata = json.load(f)
        
        self.feature_columns = metadata['feature_columns']
        self.ensemble_weights = metadata['ensemble_weights']
        self.validation_scores = metadata['validation_scores']
        self.feature_importance = metadata['feature_importance']
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        for model_name in self.ensemble_weights.keys():
            if model_name == 'neural_network':
                self.models[model_name] = keras.models.load_model(
                    os.path.join(self.model_dir, f'{model_name}.h5')
                )
            else:
                self.models[model_name] = joblib.load(
                    os.path.join(self.model_dir, f'{model_name}.pkl')
                )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑: {self.model_dir}")

def time_series_split_validation(df: pd.DataFrame, predictor: EnhancedTennisPredictor, 
                                n_splits: int = 5) -> Dict:
    """
    –ù–û–í–û–ï: –í–∞–ª–∏–¥–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    """
    print("‚è∞ –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π...")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    df_sorted = df.sort_values('match_date').copy()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X = predictor.prepare_features(df_sorted)
    y = df_sorted['won_at_least_one_set']
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"üìä –§–æ–ª–¥ {fold + 1}/{n_splits}")
        
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
        fold_predictor = EnhancedTennisPredictor()
        
        # –û–±—É—á–∞–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        fold_predictor.train_ensemble_models(X_train_cv, y_train_cv, X_val_cv, y_val_cv)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        ensemble_pred = fold_predictor.predict_probability(X_val_cv)
        fold_auc = roc_auc_score(y_val_cv, ensemble_pred)
        
        cv_scores.append(fold_auc)
        print(f"‚úÖ –§–æ–ª–¥ {fold + 1} AUC: {fold_auc:.4f}")
    
    print(f"\nüìà –°—Ä–µ–¥–Ω–∏–π AUC –ø–æ —Ñ–æ–ª–¥–∞–º: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")
    
    return {
        'mean_auc': np.mean(cv_scores),
        'std_auc': np.std(cv_scores),
        'fold_scores': cv_scores
    }

def main():
    """
    –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø: –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    """
    print("üéæ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –¢–ï–ù–ù–ò–°–ù–´–• –ú–ê–¢–ß–ï–ô")
    print("=" * 70)
    print("üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("‚Ä¢ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ 5 –º–æ–¥–µ–ª–µ–π (NN, XGBoost, RF, GB, LR)")
    print("‚Ä¢ –£–ª—É—á—à–µ–Ω–Ω–∞—è feature engineering")
    print("‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è")
    print("‚Ä¢ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print("=" * 70)
    
    # –ü—Ä–∏–º–µ—Ä —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∑–∫–∞ –≤–∞—à–∏—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    # df = pd.read_csv('enhanced_tennis_dataset.csv')
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    np.random.seed(42)
    n_samples = 5000
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    data = {
        'player_rank': np.random.exponential(50, n_samples),
        'opponent_rank': np.random.exponential(50, n_samples),
        'player_age': np.random.normal(26, 4, n_samples),
        'opponent_age': np.random.normal(26, 4, n_samples),
        'player_recent_win_rate': np.random.beta(2, 2, n_samples),
        'player_surface_advantage': np.random.normal(0, 0.1, n_samples),
        'h2h_win_rate': np.random.beta(2, 2, n_samples),
        'total_pressure': np.random.uniform(0, 4, n_samples),
        'player_form_trend': np.random.normal(0, 0.2, n_samples),
        'match_date': pd.date_range('2020-01-01', '2024-12-31', periods=n_samples)
    }
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
    strength_factor = (
        (1 / (data['player_rank'] + 1)) * 0.3 +
        data['player_recent_win_rate'] * 0.25 +
        data['player_surface_advantage'] * 0.15 +
        data['h2h_win_rate'] * 0.2 +
        data['player_form_trend'] * 0.1
    )
    
    data['won_at_least_one_set'] = np.random.binomial(1, np.clip(strength_factor, 0.1, 0.9), n_samples)
    
    df = pd.DataFrame(data)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {df['won_at_least_one_set'].value_counts().to_dict()}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏
    cutoff_date = df['match_date'].quantile(0.8)
    train_df = df[df['match_date'] < cutoff_date]
    test_df = df[df['match_date'] >= cutoff_date]
    
    print(f"üìÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"üìÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = EnhancedTennisPredictor()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    X_train = predictor.prepare_features(train_df)
    y_train = train_df['won_at_least_one_set']
    
    X_test = predictor.prepare_features(test_df)
    y_test = test_df['won_at_least_one_set']
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/validation
    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
    print("\nüèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...")
    performance = predictor.train_ensemble_models(X_train_split, y_train_split, X_val_split, y_val_split)
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_results = predictor.evaluate_models(X_test, y_test)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    predictor.plot_model_comparison(test_results)
    predictor.plot_feature_importance()
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    print("\n‚è∞ –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è...")
    cv_results = time_series_split_validation(train_df, predictor, n_splits=3)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    predictor.save_models()
    
    print("\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 50)
    best_model = max(test_results.keys(), key=lambda x: test_results[x]['auc'])
    print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")
    print(f"üìä AUC: {test_results[best_model]['auc']:.4f}")
    print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {test_results[best_model]['accuracy']:.4f}")
    print(f"‚è∞ CV AUC: {cv_results['mean_auc']:.4f} ¬± {cv_results['std_auc']:.4f}")
    
    print(f"\nüíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {predictor.model_dir}")
    print("üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –±–æ–µ–≤—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö!")

if __name__ == "__main__":
    main()